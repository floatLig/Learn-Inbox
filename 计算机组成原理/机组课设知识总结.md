# 计算机组成原理

### 基础概念
- 地址寄存器（Addre Register）：用来保存当前CPU所访问的内存单元的地址。由于在内存和CPU之间存在着操作速度上的差别，所以必须使用地址寄存器来保存地址信息，直到内存的读/写操作完成为止
- 程序存储器：通常是只读存储器，用于保存应用程序代码，同时还可以用于保存程序执行时用到的数据（例如保存查表信息）
- 指令寄存器（Instruction Register）：是临时放置在内存里面取得的程序指令的寄存器，用于存放当前从主存储器读出的正在执行的一条指令。
当执行一条指令时，先把它从内存取到数据寄存器（DR，Data Register）中，然后再传送至IR。指令划分为操作码和地址码字段，由二进制数字组成。为了执行任何给定的指令，必须对操作码进行测试，以便识别所要求的操作。指令译码器就是做这项工作的。指令寄存器中操作码字段的输出就是指令译码器的输入。操作码一经译码后，即可向操作控制器发出具体操作的特定信号。
- uPC:在采用增量方式的微指令中，形成下一条微指令地址
- 微程序控制器
	- 微程序控制器是一种控制器，同组合逻辑控制器相比较，具有规整性、灵活性、可维护性等一系列优点，因而在计算机设计中逐渐取代了早期采用的组合逻辑控制器，并已被广泛地应用。在计算机系统中，微程序设计技术是利用软件方法来设计硬件的一门技术 。
	- 微命令：控制部件通过控制线向执行部件发出的各种控制命令。它构成控制信号的最小单元
	- 微操作：执行部件接受微命令后所进行的操作。它是由微命令实现的最基本操作
	- 微指令，在机器的一个CPU周期中，一组实现一定操作功能的微命令的组合。
	- 微程序，实现一条机器指令功能的许多条微指令组成的序列。

###总线规则：

格式：奇/偶 -> 奇/偶

规则1：箭头前面的奇/偶判断【一般情况，有没有特殊情况我没有去了解】是由目的寄存器or源寄存器判断：目的寄存器为偶，源寄存器为奇；
规则2：箭头后面的奇/偶判断【一般情况】是存放的寄存器（包括AL、AH、BL、BH、CL、CH、BL、BH、IO的高八位、IO的低8位等）的高八位或者低八位位来判断：高八位是奇，低八位是偶；
举个例子：SUB r0,r1; 这里的规则是 r0 sub r1存放到r0（CL上面）寄存器上,sub(运算)前面的r0的奇偶判断是由“目标寄存器or源寄存器判断”，而这里r0是目的寄存器，所以由规则1得到是“偶”；存放到r0(CL上面)是箭头后面，且为低八位，所以由规则2得到是“偶”；故该总线规则是“偶->偶”

# 计算机组成与原理

### 冯·诺依曼体系结构：计算机组成的金字塔

**计算机的基本硬件组成**

有三大件:CPU、内存和主板
- CPU:中央处理器
（Central	Processing	Unit）,是计算机最重要的核心配件,为什么说CPU是“最重要”的呢？因为计算机的所有“计算”都是由CPU来进行的。
- 内存（Memory）:。你撰写的程序、打开的浏览器、运行的游戏，都要加载到内存
里才能运行。程序读取的数据、计算得到的结果，也都要放在内存里。内存越大，能加载的东西自然也就越
多。;举个例子，大家的电脑的内存一般为4GB、8GB、16GB等
- 主板（Motherboard）：存放在内存里的程序和数据，需要被CPU读取，CPU计算完之后，还要把数据写回到内存。然而CPU不能直接插到内存上，反之亦然。于是，就带来了最后一个大件——主板
主板是一个有着各种各样，有时候多达数十乃至上百个插槽的配件。我们的CPU要插在主板上，内存也要插
在主板上。
主板的**芯片组**（Chipset）和**总线**（Bus）解决了CPU和内存之间如何通信的问题。
芯片组：控制了数据传输的流转，也就是数据从哪里到哪里的问题。
总线则是实际数据传输的高速公路。总线速度（Bus Speed）决定了数据能传输得多快。

其他部件：I/O设备，硬盘，电源，机箱，显卡

- 输入（Input）/输出（Output）设备：输出设备：显示器；输入设备：鼠标和键盘
- 硬盘：各种数据才能持久地保存下来；举个例子，按照不同的分区，我们电脑硬盘可以分为：C盘D盘E盘等，按照不同的存储数据的介质上来看，硬盘可以分为机械硬盘（Hard Disk Drive，HDD）和固态硬盘（Solid State Disk，SSD），机械硬盘采用磁性碟片来存储数据，而固态硬盘通过闪存颗粒来存储数据。
- 显卡（Graphics	Card）：如果你用计算机玩游戏，做图形渲染或者跑深度学习应用，你多半就需要买一张单独的显卡，插在主板上。显卡之所以特殊，是因为显卡里有除了CPU之外的另一个“处理器”，也就是GPU（Graphics	Processing	Unit，图形处理器），GPU一样可以做各种“计算”的工作。

芯片组

- 北桥【其中北桥芯片起着主导性的作用，也称为主桥（Host Bridge）。】提供对CPU类型和主频的支持、系统高速缓存的支持、主板的系统总线频率、内存管理（内存类型、容量和性能）、显卡插槽规格，ISA/PCI/AGP插槽、ECC纠错等支持；
- 南桥（SouthBridge）芯片组：鼠标、键盘以及硬盘，这些都是插在主板上的。作为外部I/O设备

### 学习地图

计算机组成原理Computer Organization：计算机的基本组成、计算机的指令和计算、处理器设计，以及存储器和I/O设备
- 计算机的基本组成：冯·诺依曼体系结构中的，也就是运算器、控制器、存储器、输入设备和输出设备这五大基本组件；此外，还需要了解计算机的两个核心指标，性能和功耗
- 计算机的指令和计算：而这一条条指令执行的控制过程，就是由计算机五大组件之一的控制器来控制的实现这些运算功能的ALU（Arithmetic LogicUnit/ALU），也就是算术逻辑单元，其实就是我们计算机五大组件之一的运算器
- CPU的设计：CPU时钟可以用来构造寄存器和内存的锁存器和触发器，
- 存储器的原理：握从上到下的CPU高速缓存、内存、SSD硬盘和机械硬盘的工作原理

### 通过CPU主频，谈谈“性能”

性能：其实和我们干体力劳动很像，好比是我们要搬东西
- 响应时间（Response	time）或者叫执行时间（Execution	time）。想要提升响应时间这个性能指标，你可以理解为让计算机“跑得更快”。
- 吞吐率（Throughput）或者带宽（Bandwidth），想要提升这个指标，你可以理解为让计算机“搬得更多”。提升很容易，多堆一些硬件，比如用多核的处理器，但是提升性能就没有那么容易了
- 性能 = 1/响应时间
- 但是，响应时间会受到很多因素的影响：计算机可能同时运行着好多个程序，CPU实际上不停地在各个程序之间进行切换。在这些走掉的时间里面，很可能CPU切换去运行别的程序了。而且，有些程序在运行的时候，可能要从网络、硬盘去读取数据，要等网络和硬盘把数据读出来，给到内存和CPU。
- 故我们使用计算机的计时单位：CPU时钟
把程序的CPU执行时间变成 CPU时钟周期数（CPU Cycles）和 时钟周期时间（Clock Cycle）的乘积。
程序的CPU执行时间=CPU时钟周期数×时钟周期时间
- CPU的主频，比如我手头的这台电脑就是Intel Core-i7-7700HQ 2.8GHz，这里的2.8GHz就是电脑的主频（Frequency/Clock Rate）。这个2.8GHz，我们可以先粗浅地认为，CPU在1秒时间内，可以执行的简单指令的数量是2.8G条。
如果想要更准确一点描述，这个2.8GHz就代表，我们CPU的一个“钟表”能够识别出来的最小的时间间隔。
在我这个2.8GHz的CPU上，这个时钟周期时间，就是1/2.8G。我们的CPU，是按照这个“时钟”提示的时间来进行自己的操作。主频越高，意味着这个表走得越快，我们的CPU也就“被逼”着走得越快。时钟周期时间，就是计算机主频，这个取决于计算机硬件。我们所熟知的摩尔定律就一直在不停地提高我们计算机的主频
- 对于CPU时钟周期数，我们可以再做一个分解，把它变成“指令数×每条指令的平均时钟周期数（Cycles	Per Instruction，简称CPI）”现代的CPU通过流水线技术（Pipeline），让一条指令需要的CPU	Cycle尽可能地少程序的CPU执行时间
- 指令数×CPI×Clock	Cycle	Time这个很多时候就把挑战交给了编译器；还有我们自己打的代码

### 穿越功耗墙，我们该从哪些方面提升“性能”？

CPU的性能：程序的CPU执行时间	=	指令数×CPI×Clock	Cycle	Time

我们的CPU，一般都被叫作超大规模集成电路（Very-Large-Scale Integration，VLSI）。这些电路，实际上都是一个个晶体管组合而成的。CPU在计算，其实就是让晶体管里面的“开关”不断地去“打开”和“关闭”，来组合完成各种运算和功能。

- 功耗公式：功耗 ~= 1/2 ×负载电容×电压的平方×开关频率×晶体管数量
- 提升性能一：提升吞吐率：
1.同样的面积里面，多放一些晶体管，也就是增加密度，在这里，同样的面积下，我们想要多放一点晶体管，就要把晶体管造得小一点。这个就是平时我们所说的提升“制程”。从28nm7nm，相当于晶体管本身变成了原来的1/4大小。
2.并行提升性能，开始推出Core	Duo这样的多核CPU，通过提升“吞吐率”而不是“响应时间”，来达到目的
- 提升性能二：提升主频
- 提升性能三：功耗增加太多，就会导致CPU散热跟不上，这时，我们就需要降低电压。这里有一点非常关键，在整个功耗的公式里面，功耗和电压的平方是成正比的。这意味着电压下降到原来的1/5，整个的功耗会变成原来的1/25
- 提升性能四：.加速大概率事件。最典型的就是，过去几年流行的深度学习，整个计算过程中，99%都是向量和矩阵计算，于是，工程师们通过用GPU替代CPU，大幅度提升了深度学习的模型训练过程。本来一个CPU需要跑几小时甚至几天的程序，GPU只需要几分钟就好了。Google更是不满足于GPU的性能，进一步地推出了TPU
- 提升性能五：通过流水线提高性能
- 提升性能六：通过预测提高性能。

### 一起学习

- PC：存放当前预执行指令的地址，单字节指令执行完之后PC会+1，但是像JMP这样的三字节指令执行完PC是会+3的
- IR：存放当前的指令
如何区分指令跟数据：指令周期的不同阶段 一条指令有4个周期：取指、间指、执行、中断
计算机硬件主要通过不同的时间段来区分指令和数据,即:取指周期(或取指微程序)取出的为指令，虽然指令和数据存放的格式一样，但是访问他们的时机不同。在取指令时期，cpu通过指令流取指令，存放在指令寄存器， 然后解释并执行指令；在执行指令时期，cpu通过数据流取数据， 存放在数据寄存器。 所以指令流取的是指令，数据流取的是数据。
- CPI（Circle Per Instruction）：执行一条指令需要的时间
- 冒险
数据冒险：A等待B
结构冒险：多条指令同时访问同一硬件资源
控制冒险：由于转移指令和其他改变PC值的指令而造成的断流
- 真值：像我们看到的数，+8这样子
机器值：用二进制表示 比如说100代表+8 
- 补码：负数去补码：符号位不变，其他的取反再加一；正数的补码不变

- 二进制编码规则：

**世界上最早使用的字符集为：ASCII**，但是ASCII只有128个字符，仅仅只能表示英文，世界上的语言那么多，仅仅用ASCII来表示肯定就不能的····于是世界上就出现了多种编码方式，用来表示不同的语言。

世界上存在着多种编码方式，同一个二进制数字可以被解释成不同的符号。因此，要想打开一个文本文件，就必须知道它的编码方式，否则用错误的编码方式解读，就会出现乱码。为什么电子邮件常常出现乱码？就是因为发信人和收信人使用的编码方式不一样。

可以想象，如果有一种编码，将世界上所有的符号都纳入其中。每一个符号都给予一个独一无二的编码，那么乱码问题就会消失。**这就是 Unicode，就像它的名字都表示的，这是一种所有符号的编码。**

这里就有两个严重的问题，第一个问题是，如何才能区别 Unicode 和 ASCII ？计算机怎么知道三个字节表示一个符号，而不是分别表示三个符号呢？第二个问题是，我们已经知道，英文字母只用一个字节表示就够了，如果 Unicode 统一规定，每个符号用三个或四个字节表示，那么每个英文字母前都必然有二到三个字节是0，这对于存储来说是极大的浪费，文本文件的大小会因此大出二三倍，这是无法接受的。

因此，为了解决Unicode存储的字符占用的字节长度过长的问题，就出现了**UTF-8，UTF-8 是 Unicode 的实现方式之一**，他对Unicode的解决方式是：
1）对于单字节的符号，字节的第一位设为0，后面7位为这个符号的 Unicode 码。因此对于英语字母，UTF-8 编码和 ASCII 码是相同的。
2）对于n字节的符号（n > 1），第一个字节的前n位都设为1，第n + 1位设为0，后面字节的前两位一律设为10。剩下的没有提及的二进制位，全部为这个符号的 Unicode 码。

下面，还是以汉字严为例，演示如何实现 UTF-8 编码。
严的 Unicode 是4E25（100111000100101），根据上表，可以发现4E25处在第三行的范围内（0000 0800 - 0000 FFFF），因此严的 UTF-8 编码需要三个字节，即格式是1110xxxx 10xxxxxx 10xxxxxx。然后，从严的最后一个二进制位开始，依次从后向前填入格式中的x，多出的位补0。这样就得到了，严的 UTF-8 编码是11100100 10111000 10100101，转换成十六进制就是E4B8A5。

UTF-8是Unicode的实现方式之一，但是，这也是能算是一般情况，**简体中文常见的编码方式是 GB2312，使用两个字节表示一个汉字**，所以理论上最多可以表示 256 x 256 = 65536 个符号

- 地址码位数和地址空间大小的关系
在上面已经提到了地址总线，在计算机中 CPU的地址总线数目 决定了CPU 的 寻址 范围，这种由地址总线对应的地址称作为物理地址。假如CPU有32根地址总线（一般情况下32位的CPU的地址总线是32位，也有部分32位的CPU地址总线是36位的，比如用做服务器的CPU），那么提供的可寻址物理地址范围 为 232=4GB（在这里要注意一点，我们平常所说的32位CPU和64位CPU指的是CPU一次能够处理的数据宽度，即位宽，不是地址总线的数目）。自从64位CPU出现之后，一次便能够处理64位的数据了，其地址总线一般采用的是36位或者40位（即CPU能够寻址的物理地址空间为64GB或者1T）。